I'll guide you through setting up OpenTelemetry in your RKE2 cluster for comprehensive observability (traces, metrics, and logs).

## Complete OpenTelemetry Setup for RKE2

### Architecture Overview
We'll set up:
- **OpenTelemetry Collector** - Receives, processes, and exports telemetry
- **OpenTelemetry Operator** - Manages auto-instrumentation
- **Jaeger** - For distributed tracing
- **Integration with existing Prometheus/Grafana**

---

## Step 1: Install cert-manager (Required for Operator)

```bash
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.2/cert-manager.yaml
```

Wait for cert-manager to be ready:
```bash
kubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=cert-manager -n cert-manager --timeout=300s
```

---

## Step 2: Install OpenTelemetry Operator

```bash
kubectl apply -f https://github.com/open-telemetry/opentelemetry-operator/releases/latest/download/opentelemetry-operator.yaml
```

Verify installation:
```bash
kubectl get pods -n opentelemetry-operator-system
```

---

## Step 3: Install Jaeger for Tracing

```bash
# Create namespace
kubectl create namespace observability

# Install Jaeger Operator
kubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.51.0/jaeger-operator.yaml -n observability
```

Wait for Jaeger operator:
```bash
kubectl wait --for=condition=ready pod -l name=jaeger-operator -n observability --timeout=300s
```

**Deploy Jaeger instance:**

```yaml
# jaeger-instance.yaml
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger
  namespace: observability
spec:
  strategy: allInOne
  allInOne:
    image: jaegertracing/all-in-one:latest
    options:
      log-level: debug
  storage:
    type: memory
    options:
      memory:
        max-traces: 100000
  ingress:
    enabled: false
  ui:
    options:
      dependencies:
        menuEnabled: true
      tracking:
        gaID: UA-000000-2
```

Apply:
```bash
kubectl apply -f jaeger-instance.yaml
```

---

## Step 4: Deploy OpenTelemetry Collector

Apply the configuration:

```yaml
# otel-collector-config.yaml
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
  namespace: observability
spec:
  mode: deployment
  replicas: 1
  ports:
    - name: prometheus
      port: 8889
      targetPort: 8889
      protocol: TCP
    - name: metrics
      port: 8888
      targetPort: 8888
      protocol: TCP
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
    processors:
      batch: {}
    exporters:
      otlp/jaeger:
        endpoint: jaeger-collector.observability.svc.cluster.local:4317
        tls:
          insecure: true
      prometheus:
        endpoint: "0.0.0.0:8889"
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp/jaeger]
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [prometheus]
```

```bash
kubectl apply -f otel-collector-config.yaml
```

Verify the collector is running:
```bash
kubectl get pods -n observability -l app.kubernetes.io/component=opentelemetry-collector
```

---

## Step 5: Configure Auto-Instrumentation

Set up auto-instrumentation for different languages:

```yaml
# otel-auto-instrumentation.yaml
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: java-instrumentation
  namespace: observability
spec:
  exporter:
    endpoint: http://otel-collector.observability.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: "0.25"
  java:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-java:latest
---
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: python-instrumentation
  namespace: observability
spec:
  exporter:
    endpoint: http://otel-collector.observability.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: "0.25"
  python:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-python:latest
---
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: nodejs-instrumentation
  namespace: observability
spec:
  exporter:
    endpoint: http://otel-collector.observability.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: "0.25"
  nodejs:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-nodejs:latest
---
apiVersion: opentelemetry.io/v1beta1
kind: Instrumentation
metadata:
  name: dotnet-instrumentation
  namespace: observability
spec:
  exporter:
    endpoint: http://otel-collector.observability.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: "0.25"
  dotnet:
    image: ghcr.io/open-telemetry/opentelemetry-operator/autoinstrumentation-dotnet:latest
```

Apply auto-instrumentation:
```bash
kubectl apply -f otel-auto-instrumentation.yaml
```

---

## Step 6: Instrument Your Applications

### Method 1: Using Auto-Instrumentation (Easiest)

Add annotation to your deployment to enable auto-instrumentation:

```yaml
# example-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-dotnet-app
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: my-dotnet-app
  template:
    metadata:
      labels:
        app: my-dotnet-app
      annotations:
        # Enable .NET auto-instrumentation
        instrumentation.opentelemetry.io/inject-dotnet: "observability/dotnet-instrumentation"
        # For Java use:
        # instrumentation.opentelemetry.io/inject-java: "observability/java-instrumentation"
        # For Python use:
        # instrumentation.opentelemetry.io/inject-python: "observability/python-instrumentation"
        # For Node.js use:
        # instrumentation.opentelemetry.io/inject-nodejs: "observability/nodejs-instrumentation"
    spec:
      containers:
      - name: my-dotnet-app
        image: your-registry/my-dotnet-app:latest
        ports:
        - containerPort: 8080
        env:
        - name: OTEL_SERVICE_NAME
          value: my-dotnet-app
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: service.namespace=default,service.version=1.0.0
```

### Method 2: Manual Instrumentation (For More Control)

**Example: .NET Minimal API Application**

```csharp
// Program.cs
using OpenTelemetry;
using OpenTelemetry.Trace;
using OpenTelemetry.Metrics;
using OpenTelemetry.Resources;

var builder = WebApplication.CreateBuilder(args);

// Add OpenTelemetry
builder.Services.AddOpenTelemetry()
    .ConfigureResource(resource => resource
        .AddService(serviceName: "my-dotnet-app", serviceVersion: "1.0.0"))
    .WithTracing(tracing => tracing
        .AddAspNetCoreInstrumentation()
        .AddHttpClientInstrumentation()
        .AddOtlpExporter(options =>
        {
            options.Endpoint = new Uri("http://otel-collector.observability.svc.cluster.local:4318/v1/traces");
        }))
    .WithMetrics(metrics => metrics
        .AddAspNetCoreInstrumentation()
        .AddHttpClientInstrumentation()
        .AddOtlpExporter(options =>
        {
            options.Endpoint = new Uri("http://otel-collector.observability.svc.cluster.local:4318/v1/metrics");
        }));

var app = builder.Build();

app.MapGet("/api/users", () =>
{
    return new[] { "Alice", "Bob" };
});

app.Run();
```

**Dockerfile for .NET Minimal API:**

```dockerfile
# Use the official .NET 8 SDK image to build the app
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src

# Copy the project file and restore dependencies
COPY ["MyDotNetApp.csproj", "./"]
RUN dotnet restore "./MyDotNetApp.csproj"

# Copy the rest of the source code
COPY . .
RUN dotnet build "MyDotNetApp.csproj" -c Release -o /app/build

# Publish the app
FROM build AS publish
RUN dotnet publish "MyDotNetApp.csproj" -c Release -o /app/publish

# Use the official .NET 8 runtime image
FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS final
WORKDIR /app
COPY --from=publish /app/publish .

EXPOSE 8080
ENTRYPOINT ["dotnet", "MyDotNetApp.dll"]
```

---

## Step 7: Configure ServiceMonitor for Prometheus

Create a ServiceMonitor to scrape OpenTelemetry metrics:

```yaml
# servicemonitor-otel.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: otel-collector
  namespace: observability
  labels:
    app: opentelemetry
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: otel-collector
  endpoints:
  - port: prometheus
    interval: 30s
    path: /metrics
  - port: metrics
    interval: 30s
    path: /metrics
```

Apply:
```bash
kubectl apply -f servicemonitor-otel.yaml
```

---

## Step 8: Access Jaeger UI

**Port forward Jaeger UI:**
```bash
kubectl port-forward -n observability svc/jaeger-query 16686:16686
```

Access Jaeger at: `http://localhost:16686`

**Or expose via NodePort:**
```bash
kubectl patch svc jaeger-query -n observability -p '{"spec": {"type": "NodePort"}}'
kubectl get svc -n observability jaeger-query
```

---

## Step 9: Integrate with Grafana

### Add Jaeger as Data Source in Grafana

1. Port forward Grafana:
```bash
kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80
```

2. Login to Grafana (`http://localhost:3000`)

3. Go to **Configuration** → **Data Sources** → **Add data source**

4. Select **Jaeger**

5. Configure:
   - **URL**: `http://jaeger-query.observability.svc.cluster.local:16686`
   - Click **Save & Test**

### Import OpenTelemetry Dashboard---

## Step 10: Test OpenTelemetry Setup

### Deploy a Test Application

```yaml
# otel-test-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-test-app
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-test-app
  template:
    metadata:
      labels:
        app: otel-test-app
      annotations:
        instrumentation.opentelemetry.io/inject-nodejs: "observability/nodejs-instrumentation"
    spec:
      containers:
      - name: otel-test-app
        image: node:18-alpine
        command: ["node", "-e", "const express = require('express'); const app = express(); app.get('/', (req, res) => res.send('Hello OpenTelemetry!')); app.listen(3000);"]
        ports:
        - containerPort: 3000
---
apiVersion: v1
kind: Service
metadata:
  name: otel-test-app
  namespace: default
spec:
  selector:
    app: otel-test-app
  ports:
    - port: 80
      targetPort: 3000
  type: ClusterIP
```

Deploy test application:
```bash
kubectl apply -f otel-test-app.yaml
```

---

## Step 11: Verify Everything is Working

### Check OpenTelemetry Collector Logs
```bash
kubectl logs -n observability -l app.kubernetes.io/component=opentelemetry-collector --tail=100
```

### Check if traces are being received
```bash
kubectl logs -n observability -l app.kubernetes.io/component=opentelemetry-collector | grep "traces"
```

### View Traces in Jaeger
1. Access Jaeger UI: `http://localhost:16686`
2. Select your service from the dropdown
3. Click "Find Traces"

### Check Metrics in Prometheus
```bash
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
```

Visit `http://localhost:9090` and query:
```promql
otel_http_server_duration_count
```

---

## Step 12: Configure Sampling (Optional)

To reduce data volume, configure sampling:

```yaml
# sampling-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-sampling
  namespace: observability
data:
  sampling.yaml: |
    # Tail-based sampling
    processors:
      tail_sampling:
        decision_wait: 10s
        num_traces: 100
        expected_new_traces_per_sec: 10
        policies:
          # Sample all error traces
          - name: error-policy
            type: status_code
            status_code:
              status_codes:
                - ERROR
          
          # Sample 10% of successful traces
          - name: probabilistic-policy
            type: probabilistic
            probabilistic:
              sampling_percentage: 10
          
          # Always sample slow requests (>1s)
          - name: latency-policy
            type: latency
            latency:
              threshold_ms: 1000
```

---

## Troubleshooting

### 1. Collector not receiving traces
```bash
# Check collector service
kubectl get svc -n observability otel-collector

# Check endpoints
kubectl get endpoints -n observability otel-collector

# Test connectivity from a pod
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -- \
  curl -v http://otel-collector.observability.svc.cluster.local:4318/v1/traces
```

### 2. Auto-instrumentation not working
```bash
# Check if operator is running
kubectl get pods -n opentelemetry-operator-system

# Check instrumentation resources
kubectl get instrumentation -n observability

# Check pod annotations
kubectl describe pod <pod-name> -n default
```

### 3. Jaeger not showing traces
```bash
# Check Jaeger pods
kubectl get pods -n observability -l app=jaeger

# Check Jaeger logs
kubectl logs -n observability -l app.kubernetes.io/component=query

# Verify collector is exporting to Jaeger
kubectl logs -n observability -l app.kubernetes.io/component=opentelemetry-collector | grep jaeger
```

---

## Useful Commands

```bash
# View all OpenTelemetry resources
kubectl get opentelemetrycollector,instrumentation -A

# Check collector metrics
kubectl port-forward -n observability svc/otel-collector 8888:8888
curl http://localhost:8888/metrics

# View trace samples
kubectl logs -n observability -l app.kubernetes.io/component=opentelemetry-collector | grep "Trace ID"

# Scale collector
kubectl scale deployment -n observability otel-collector --replicas=3

# Update collector config
kubectl edit opentelemetrycollector -n observability otel-collector
```

---

## Architecture Summary

```
┌─────────────────┐
│  Applications   │
│  (Instrumented) │
└────────┬────────┘
         │ OTLP (gRPC/HTTP)
         ▼
┌──────────────────────┐
│ OpenTelemetry        │
│ Collector            │
│ - Receives traces    │
│ - Processes metrics  │
│ - Forwards logs      │
└────┬──────────┬──────┘
     │          │
     ▼          ▼
┌────────┐  ┌──────────┐
│ Jaeger │  │Prometheus│
│(Traces)│  │(Metrics) │
└────────┘  └──────────┘
     │          │
     └────┬─────┘
          ▼
    ┌──────────┐
    │ Grafana  │
    │(Visualize)│
    └──────────┘
```

Your OpenTelemetry setup is now complete! You have full observability with traces, metrics, and logs collection across your RKE2 cluster.